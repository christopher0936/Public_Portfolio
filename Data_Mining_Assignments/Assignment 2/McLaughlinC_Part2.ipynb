{"cells":[{"cell_type":"markdown","metadata":{"id":"x-qg3dmgseoP"},"source":["# Student: McLaughlin, Chris\n","\n","# Problem 2. Support machines."]},{"cell_type":"markdown","metadata":{"id":"XRlUhg4_I0tD"},"source":["## Notes/README\n","- Notebook is heavy on memory usage and MUST be run in an environment with sufficient RAM - Assignment was done in Google Collab!"]},{"cell_type":"markdown","metadata":{"id":"_CDArHnpI0tE"},"source":["## Acknowledgements/Citations\n","\n","- Lab 5 notebook as a resource on SVMs in scikit-learn\n","- https://stackoverflow.com/questions/17071871/how-do-i-select-rows-from-a-dataframe-based-on-column-values\n","- https://stackoverflow.com/questions/36921951/truth-value-of-a-series-is-ambiguous-use-a-empty-a-bool-a-item-a-any-o\n","- https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n","- https://stackoverflow.com/questions/68300381/is-scikit-learns-support-vector-classifier-hard-margin-or-soft-margin\n","- https://stackoverflow.com/questions/12355434/svm-with-hard-margin-and-c-value\n","- https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html\n","- https://scikit-learn.org/stable/modules/cross_validation.html\n","- https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html\n","- https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n","- https://scikit-learn.org/stable/modules/svm.html\n","- https://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsOneClassifier.html"]},{"cell_type":"markdown","metadata":{"id":"3lKrTRleI0tF"},"source":["## Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"llVFdinbseoS"},"outputs":[],"source":["# This should be roughly the content of the first code cell\n","import numpy as np\n","import random\n","np.random.seed(1337)\n","random.seed(1337)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AaemlDYIseoT"},"outputs":[],"source":["# Plotting support\n","from matplotlib import pyplot as plt\n","#from plotnine import *\n","# Standard libraries\n","import pandas as pd\n","import sklearn as sk\n","from keras.datasets import mnist\n","import tensorflow as tf\n","from sklearn.svm import SVC\n","from sklearn.model_selection import cross_val_score\n","from sklearn.multiclass import OneVsOneClassifier"]},{"cell_type":"markdown","source":["### Data Import and Processing"],"metadata":{"id":"TtBnu7bvtx6v"}},{"cell_type":"code","source":["(train_X, train_y), (test_X, test_y) = mnist.load_data() # From assignment spec\n","\n","#train_X.shape\n","#test_X.shape\n","\n","# Flatten and Normalise data while casting to float32 - Same as in Problem 1, see that questions explaination and acknowledements\n","train_X = train_X.reshape(60000,784) # Reshape data from into a 2D array - each 2D 28*28 img array in the training data will be converted to a 1D 784 array\n","train_X = train_X.astype(np.float32)\n","train_X = train_X/255\n","\n","# Repeat for test data\n","test_X = test_X.reshape(10000,784)\n","test_X = test_X.astype(np.float32)\n","test_X = test_X/255\n","\n","\"\"\" OTHER METHOD - THIS WAS SLOW AS MOLLASES SO I'M TRYING SOMETHING ELSE\n","# Select 1s and 7s\n","# For train\n","newtrain_X=np.ndarray(shape=(60000,784))\n","newtrain_y=np.ndarray(60000,)\n","\n","for index in range(60000):\n","  print(index)\n","  if train_y[index]==1 or train_y[index]==7:\n","    np.append(newtrain_X,train_X[index])\n","    np.append(newtrain_y,train_y[index])\n","\n","\"\"\"\n","\n","# Merge X and y for filtering\n","newtrain=pd.DataFrame(train_X)\n","newtrain[\"RESULT\"]=train_y\n","newtest=pd.DataFrame(test_X)\n","newtest[\"RESULT\"]=test_y\n","\n","# Filter data\n","datatrain=newtrain.loc[(newtrain[\"RESULT\"]==1) | (newtrain[\"RESULT\"]==7)]\n","datatest=newtest.loc[(newtest[\"RESULT\"]==1) | (newtest[\"RESULT\"]==7)]\n","\n","#datatrain.head()\n","#datatest.head()"],"metadata":{"id":"R_R50K0qt0PO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["What we've done here is import the MNIST dataset and filter it into two pandas dataframes (train and test) which contain 784 data columns and one result column!"],"metadata":{"id":"dzu1t62g2HEW"}},{"cell_type":"markdown","source":["## a) Hard"],"metadata":{"id":"PjDcDr_uvF8j"}},{"cell_type":"markdown","source":["We're going to train a hard-margin SVM classifier (implemented via an extremely high C hyperparameter and linear kernel) that determines whether a character is a 1 or a 7"],"metadata":{"id":"Qe_xhFVdvIvo"}},{"cell_type":"code","source":["hardsvm = SVC(C=1e20, kernel=\"linear\").fit(datatrain.drop(columns=[\"RESULT\"]),datatrain[\"RESULT\"])\n","score_train = hardsvm.score(datatrain.drop(columns=[\"RESULT\"]),datatrain[\"RESULT\"])\n","score_test = hardsvm.score(datatest.drop(columns=[\"RESULT\"]),datatest[\"RESULT\"])\n","print(\"Training Score:\", score_train, \"\\nTesting Score:\", score_test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ujL4STe37nGg","executionInfo":{"status":"ok","timestamp":1667708255045,"user_tz":420,"elapsed":2466,"user":{"displayName":"Chris McLaughlin","userId":"07791563133797571791"}},"outputId":"d524b8fd-96a5-4ef6-a4da-724e18b94714"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training Score: 1.0 \n","Testing Score: 0.9916782246879334\n"]}]},{"cell_type":"markdown","source":["And we regularly get over 99% Testing accuracy! That's pretty impressive!"],"metadata":{"id":"64fZZNRv-PeN"}},{"cell_type":"markdown","source":["## b) Soft"],"metadata":{"id":"EE5-zGjN-Ycf"}},{"cell_type":"markdown","source":["We're going to repeat the above, but use a soft-margin SVM with varying hyperparameter C - we will select the best C by running several options and validating each with 5-fold cross-validation!"],"metadata":{"id":"nuuudzAaCgcU"}},{"cell_type":"code","source":["c_vals = [0.1,0.5,1,10,50]\n","for c in c_vals:\n","  softsvm = SVC(C=c, kernel=\"linear\") #scores_crossval automatically takes care of fitting :D\n","  scores_crossval = cross_val_score(softsvm, datatrain.drop(columns=[\"RESULT\"]), datatrain[\"RESULT\"], cv=5) # see cross_val_score documentation\n","  print(\"C hyperparameter:\", c, \"\\tCross-Validated Accuracy Scores:\", scores_crossval, \"\\tAverage Score with this C:\", sum(scores_crossval)/len(scores_crossval))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xldrdih3-bPs","executionInfo":{"status":"ok","timestamp":1667708282540,"user_tz":420,"elapsed":27497,"user":{"displayName":"Chris McLaughlin","userId":"07791563133797571791"}},"outputId":"a03c5a4b-e8ac-4e47-ce17-63b629980974"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["C hyperparameter: 0.1 \tCross-Validated Accuracy Scores: [0.99654112 0.99692544 0.99653979 0.99307958 0.99615532] \tAverage Score with this C: 0.9958482532438154\n","C hyperparameter: 0.5 \tCross-Validated Accuracy Scores: [0.99654112 0.99654112 0.99538639 0.99384852 0.99500192] \tAverage Score with this C: 0.9954638152830121\n","C hyperparameter: 1 \tCross-Validated Accuracy Scores: [0.99654112 0.99538816 0.99577086 0.99346405 0.99538639] \tAverage Score with this C: 0.99531011693309\n","C hyperparameter: 10 \tCross-Validated Accuracy Scores: [0.99538816 0.99385088 0.99423299 0.99384852 0.99423299] \tAverage Score with this C: 0.9943107082624463\n","C hyperparameter: 50 \tCross-Validated Accuracy Scores: [0.99538816 0.99385088 0.99423299 0.99384852 0.99423299] \tAverage Score with this C: 0.9943107082624463\n"]}]},{"cell_type":"markdown","source":["There's surprisingly little change with different values of C, but we can see that in general the lower C value of 0.1 produces more accurate results."],"metadata":{"id":"WasI4cPnHTOe"}},{"cell_type":"markdown","source":["## c) Kernel"],"metadata":{"id":"532m_vwKHbMl"}},{"cell_type":"markdown","source":["Now we're going to try a few different kernels with our best performing SVM from above (C=0.1)!"],"metadata":{"id":"JxErgu--H7sZ"}},{"cell_type":"markdown","source":["### i) Polynomial Kernel\n"],"metadata":{"id":"YhlEQxc_Hexq"}},{"cell_type":"markdown","source":["First we'll try a polymomial kernel with a few different degrees"],"metadata":{"id":"QjtvddQGHhD-"}},{"cell_type":"code","source":["degrees = [2, 3, 4]\n","for deg in degrees:\n","  polysvm = SVC(C=0.1, kernel=\"poly\", degree=deg).fit(datatrain.drop(columns=[\"RESULT\"]),datatrain[\"RESULT\"])\n","  score_train = polysvm.score(datatrain.drop(columns=[\"RESULT\"]),datatrain[\"RESULT\"])\n","  score_test = polysvm.score(datatest.drop(columns=[\"RESULT\"]),datatest[\"RESULT\"])\n","  print(\"Degree:\", deg, \"\\tTraining Accuracy:\", score_train, \"\\tTesting Accuracy:\", score_test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jsYrIof8IKjs","executionInfo":{"status":"ok","timestamp":1667708312207,"user_tz":420,"elapsed":29677,"user":{"displayName":"Chris McLaughlin","userId":"07791563133797571791"}},"outputId":"ec42a706-a19c-4981-8f95-1062d5154630"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Degree: 2 \tTraining Accuracy: 0.9943107557469055 \tTesting Accuracy: 0.9902912621359223\n","Degree: 3 \tTraining Accuracy: 0.9931575305604674 \tTesting Accuracy: 0.9893666204345816\n","Degree: 4 \tTraining Accuracy: 0.9896978550011533 \tTesting Accuracy: 0.9833564493758669\n"]}]},{"cell_type":"markdown","source":["We can see that a second degree polynomial kernel typically produces the best testing results."],"metadata":{"id":"GZCIL4hBZH1X"}},{"cell_type":"markdown","source":["### ii) Gaussian Kernel"],"metadata":{"id":"1g44av5yLcnv"}},{"cell_type":"markdown","source":["Now we'll do a Gaussian kernel and try a few different gamma hyperparameters\n","\n","Note: Be warned, this takes a hot minute to execute!"],"metadata":{"id":"QvDH8X3kKwQA"}},{"cell_type":"code","source":["gammas = [0.1, 0.5, 1]\n","for gamma in gammas:\n","  gausvm = SVC(C=0.1, kernel=\"rbf\", gamma=gamma).fit(datatrain.drop(columns=[\"RESULT\"]),datatrain[\"RESULT\"])\n","  score_train = gausvm.score(datatrain.drop(columns=[\"RESULT\"]),datatrain[\"RESULT\"])\n","  score_test = gausvm.score(datatest.drop(columns=[\"RESULT\"]),datatest[\"RESULT\"])\n","  print(\"Gamma:\", gamma, \"\\tTraining Accuracy:\", score_train, \"\\tTesting Accuracy:\", score_test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w9-iC0ycVB8_","executionInfo":{"status":"ok","timestamp":1667708720125,"user_tz":420,"elapsed":407927,"user":{"displayName":"Chris McLaughlin","userId":"07791563133797571791"}},"outputId":"f467f7ff-0d73-4dc0-f984-4008dec0eed4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Gamma: 0.1 \tTraining Accuracy: 0.9830860305989083 \tTesting Accuracy: 0.9852057327785483\n","Gamma: 0.5 \tTraining Accuracy: 0.7950334435304067 \tTesting Accuracy: 0.7651410078594545\n","Gamma: 1 \tTraining Accuracy: 0.5183362804643653 \tTesting Accuracy: 0.5247341655108645\n"]}]},{"cell_type":"markdown","source":["And we see setting the gamma hyperparameter to 0.1 produces by *far* the best results!"],"metadata":{"id":"Ask3gpEGZNV3"}},{"cell_type":"markdown","source":["### iii) Linear Kernel"],"metadata":{"id":"qLte0BfjZW3u"}},{"cell_type":"markdown","source":["We already did a linear kernel earlier when we did our cross-validating soft svm, but we'll repeat the process with a standard svm here!"],"metadata":{"id":"EfU_-OFYZX8t"}},{"cell_type":"code","source":["linsvm = SVC(C=0.1, kernel=\"linear\").fit(datatrain.drop(columns=[\"RESULT\"]),datatrain[\"RESULT\"])\n","score_train = linsvm.score(datatrain.drop(columns=[\"RESULT\"]),datatrain[\"RESULT\"])\n","score_test = linsvm.score(datatest.drop(columns=[\"RESULT\"]),datatest[\"RESULT\"])\n","print(\"Training Accuracy:\", score_train, \"Testing Accuracy:\", score_test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qNqcugRJZm7E","executionInfo":{"status":"ok","timestamp":1667708722253,"user_tz":420,"elapsed":2138,"user":{"displayName":"Chris McLaughlin","userId":"07791563133797571791"}},"outputId":"7d106a4e-6659-489c-b021-029f2482c0e6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training Accuracy: 0.9977704313062197 Testing Accuracy: 0.9944521497919556\n"]}]},{"cell_type":"markdown","source":["And we get a *very* accurate model!"],"metadata":{"id":"oglzRfG5aSLx"}},{"cell_type":"markdown","source":["## d) AVA"],"metadata":{"id":"XSSO-zo5e8SS"}},{"cell_type":"markdown","source":["All-vs-all classification consists of creating $ n(n-1) $ pairwise (also called one-vs-one) classifiers for each class in the data. When run, the character that is most predicted by all one-vs-one classifiers is the one that is predicted by the overall all-vs-all aggregation.\n","\n","Because each pairwise classifier is a binary classifier, just like the previous SVMs that determine whether a character is a 1 or a 7, we actually only need half the total number of classifiers, or $ n(n-1)/2 $. For ten possible characters (0 through 9), this corresponds to 45 pairwise classifiers as part of our all-vs-all classifer. Scikit-learn provides an aggregator precisely for this (confusingly enough called one-vs-one, though the algorithm it employs is in fact all-vs-all, and it is merely made up of one-vs-one classfiers)\n","\n","NOTE: Warning! This one also takes a long time to run!"],"metadata":{"id":"RFgF45DDe-6N"}},{"cell_type":"code","source":["# We'll be using newtrain and newtest here since we want the full dataset!\n","# We'll use a soft-margin linear svm with a c hyperparameter of 0.1 since this has given us the best test accuracy so far\n","\n","ava = OneVsOneClassifier(\n","    SVC(C=0.1, kernel=\"linear\")\n",").fit(newtrain.drop(columns=[\"RESULT\"]), newtrain[\"RESULT\"])\n","avascore_train = ava.score(newtrain.drop(columns=[\"RESULT\"]), newtrain[\"RESULT\"])\n","avascore_test = ava.score(newtest.drop(columns=[\"RESULT\"]), newtest[\"RESULT\"])\n","print(\"Training Accuracy:\", avascore_train, \"\\tTesting Accuracy:\", avascore_test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yCrC4EsBCyay","executionInfo":{"status":"ok","timestamp":1667709227823,"user_tz":420,"elapsed":505572,"user":{"displayName":"Chris McLaughlin","userId":"07791563133797571791"}},"outputId":"868f9ff0-9df9-4c2e-deba-8ebb798f2882"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training Accuracy: 0.9579333333333333 \tTesting Accuracy: 0.947\n"]}]},{"cell_type":"markdown","source":["And we typically get around 95% testing accuracy, which, when one remembers we're working with the whole dataset and all ten possible digits here, is quite impressive!"],"metadata":{"id":"NNaT-N_eJ2eN"}}],"metadata":{"kernelspec":{"display_name":"Python 3.7.7 64-bit","language":"python","name":"python37764bit7ab3a842c56f47fc9beb3e23e6b5e94d"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}