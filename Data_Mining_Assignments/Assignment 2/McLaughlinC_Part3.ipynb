{"cells":[{"cell_type":"markdown","metadata":{"id":"mja3C7MCy5j0"},"source":["# Student: McLaughlin, Chris\n","\n","# Problem 3. Pigs, begone!"]},{"cell_type":"markdown","metadata":{"id":"XRlUhg4_I0tD"},"source":["## Notes/README\n","- Notebook is heavy on memory usage and MUST be run in an environment with sufficient RAM - Assignment was done in Google Collab!\n","- Ensure that enron5.tar.gz is present in the working directory!"]},{"cell_type":"markdown","metadata":{"id":"_CDArHnpI0tE"},"source":["## Acknowledgements/Citations\n","\n","- Lecture Slides - 12 - Bayes\n","- https://en.wikipedia.org/wiki/Naive_Bayes_classifier\n","- https://jonndata.github.io/2020-07-30-Naive-Bayes-From-Scratch/\n","- https://medium.com/@rangavamsi5/na%C3%AFve-bayes-algorithm-implementation-from-scratch-in-python-7b2cc39268b9\n","- https://stackoverflow.com/questions/18617244/read-contents-of-tar-gz-file-from-website-into-a-python-3-x-object\n","- https://stackoverflow.com/questions/26695903/python-how-to-read-all-files-in-a-directory\n","- https://www.geeksforgeeks.org/python-dictionary-keys-method/#:~:text=The%20keys()%20method%20in,order%20of%20insertion%20using%20Python.&text=Parameters%3A%20There%20are%20no%20parameters,that%20displays%20all%20the%20keys.\n","- https://www.w3schools.com/python/python_howto_remove_duplicates.asp"]},{"cell_type":"markdown","metadata":{"id":"3lKrTRleI0tF"},"source":["## Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ss_M3t0vy5j2"},"outputs":[],"source":["# This should be roughly the content of the first code cell\n","import numpy as np\n","import random\n","np.random.seed(1337)\n","random.seed(1337)\n","# for downloading and importing the data\n","import tarfile\n","from pathlib import Path"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LQ0j8U34y5j3"},"outputs":[],"source":["# Plotting support\n","from matplotlib import pyplot as plt\n","#from plotnine import *\n","# Standard libraries\n","import pandas as pd\n","#import sklearn as sk"]},{"cell_type":"markdown","source":["## Some theory before we do this"],"metadata":{"id":"lM3EYxEz19T9"}},{"cell_type":"markdown","source":["Bayesian Problem (from wikipedia and lecture slides):\n","\n","$ \\frac{P(spam|x_i)P(spam)}{P(x_i)} = \\frac{prior \\times liklihood}{evidence} $ \n","\n","where the denominator is irrelevant.\n","\n","The Naïve Bayes formula then, after applying MLE, is:\n","\n","$ P(\\text{spam}|x_i) \\propto P(x_i|\\text{spam})P(\\text{spam}) = (\\frac{\\text{number of spam emails}}{\\text{number of messages}}) ∏_{j}(\\frac{\\text{number of spam emails containing word j}}{\\text{number of spam messages}}) $\n","\n","ie. Multiply the observed probabilities of each word being present in spam emails together, and then multiply this by the probability of an email being spam in the first place. The resulting probability is proportional to $ P(spam|x_i) $, which is good enough for our purposes as we're going to construct a classifier which simply selects the class with the highest probability."],"metadata":{"id":"nzXu44pe2UiC"}},{"cell_type":"markdown","metadata":{"id":"Zx8ZrH22y5j3"},"source":["## Data Import and Processing"]},{"cell_type":"markdown","source":["We're going to split our data into 72% train and 28% test (needed for nice numbers on the split!), and represent each email as a bag of words"],"metadata":{"id":"Pptf805q76at"}},{"cell_type":"code","source":["# Extract the tar\n","tar = tarfile.open(\"enron5.tar.gz\")\n","tar.extractall(\"./temp_assignment2_files\")\n","tar.close"],"metadata":{"id":"AJFFK2SSzy34","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1667729966690,"user_tz":480,"elapsed":2241,"user":{"displayName":"Chris McLaughlin","userId":"07791563133797571791"}},"outputId":"36c5add2-e566-4704-cd9f-cae2bce8b926"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<bound method TarFile.close of <tarfile.TarFile object at 0x7faca1e56510>>"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["# Import files to two seperate lists\n","spam_emails=[]\n","ham_emails=[]\n","for thing in Path(\"./temp_assignment2_files/enron5/spam\").iterdir():\n","  if thing.is_file():\n","    spam_emails.append(thing.read_text(encoding=\"latin-1\"))\n","for thing in Path(\"./temp_assignment2_files/enron5/ham\").iterdir():\n","  if thing.is_file():\n","    ham_emails.append(thing.read_text(encoding=\"latin-1\"))"],"metadata":{"id":"FuYNBeN_BoSs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Data Splitting and Model Training"],"metadata":{"id":"XXJCT052LXOX"}},{"cell_type":"code","source":["# Do the test/train split\n","sdivider = int(len(spam_emails)*0.72)\n","hdivider = int(len(ham_emails)*0.72)\n","\n","#print(sdivider)\n","#print(hdivider)\n","\n","spam_emails_train = spam_emails[:sdivider]\n","spam_emails_test = spam_emails[sdivider:]\n","ham_emails_train = ham_emails[:hdivider]\n","ham_emails_test = ham_emails[hdivider:]\n","\n","#print(len(spam_emails_train))\n","#print(len(spam_emails_test))\n","#print(len(ham_emails_train))\n","#print(len(ham_emails_test))\n","\n","# Convert emails to lists of words\n","for index in range(len(spam_emails_train)):\n","  spam_emails_train[index] = spam_emails_train[index].split()\n","for index in range(len(ham_emails_train)):\n","  ham_emails_train[index] = ham_emails_train[index].split()\n","\n","# Figure out our observed probabilities\n","spamdict={}\n","hamdict={}\n","\n","for email in spam_emails_train:\n","  for word in email:\n","    spamdict[word]=0\n","for email in ham_emails_train:\n","  for word in email:\n","    hamdict[word]=0\n","\n","# Train the model (ie. Learn the number of spam emails and ham emails per word)\n","\"\"\" this was O(n^absolutelynot) so we're going to try something else\n","for key in spamdict.keys():\n","  for email in spam_emails_train:\n","    if key in email:\n","      spamdict[key]+=1\n","for key in hamdict.keys():\n","  for email in ham_emails_train:\n","    if key in email:\n","      hamdict[key]+=1\n","\"\"\"\n","\n","def list_unique_vals(inlist): #It's super stupid that python doesn't have a builtin list method to do this - this is from the w3schools article in the acknowledgements\n","  return list(dict.fromkeys(inlist))\n","\n","for index in range(len(spam_emails_train)):\n","  spam_emails_train[index] = list_unique_vals(spam_emails_train[index])\n","for index in range(len(ham_emails_train)):\n","  ham_emails_train[index] = list_unique_vals(ham_emails_train[index])\n","\n","for email in spam_emails_train:\n","  for word in email:\n","    spamdict[word]+=1\n","for email in ham_emails_train:\n","  for word in email:\n","    hamdict[word]+=1\n","\n","# Lets get our P(spam) and P(ham)\n","Pspam = len(spam_emails_train)/(len(spam_emails_train)+len(ham_emails_train))\n","Pham = len(ham_emails_train)/(len(ham_emails_train)+len(spam_emails_train))\n","\n","# One last thing - lets convert our dicts to probabilities rather than absolute values - this will save us having to do it in the classifier\n","for key in spamdict.keys():\n","  spamdict[key] = spamdict[key]/len(spam_emails_train)\n","for key in hamdict.keys():\n","  hamdict[key] = hamdict[key]/len(ham_emails_train)"],"metadata":{"id":"IVxWb2EKDtdX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The model is now trained! We have dictionaries for each class composed of the number of emails of that class that contain a given word, as well as Pspam. That's actually all we need to start constructing our actual classifier!"],"metadata":{"id":"DokZyegJO6Ky"}},{"cell_type":"markdown","source":["### Classifier"],"metadata":{"id":"uURRHuvRPsxv"}},{"cell_type":"markdown","source":["We'll construct our classifier function here, and then do the actual predictions later!"],"metadata":{"id":"-rbclUrbVdpH"}},{"cell_type":"code","source":["def isSpam(email):\n","  email = email.split()\n","  email = list_unique_vals(email)\n","  \n","  spam_probs = []\n","  ham_probs = []\n","\n","  # Get spam probs\n","  for word in email:\n","    if word in spamdict:\n","      spam_probs.append(spamdict[word])\n","    else:\n","      spam_probs.append(0) # Note: This is a TERRIBLE way of doing this for reasons I'll discuss in proper markup text later on - but it's by the book, the lecture slides show this being the algorithm\n","  \n","  # Get ham probs\n","  for word in email:\n","    if word in hamdict:\n","      ham_probs.append(hamdict[word])\n","    else:\n","      ham_probs.append(0)\n","\n","  # We now have our collection of probabilities (x_i|spam) and (x_i|ham)\n","\n","  # Okay, let's work out whether this email is more likely to be spam or ham\n","  # P(spam|x_i)\n","  Pspamx = Pspam\n","  for prob in spam_probs:\n","    Pspamx *= prob\n","  # P(ham|x_i)\n","  Phamx = Pham\n","  for prob in ham_probs:\n","    Phamx *= prob\n","  \n","  # Remember these are not \"true\" probabilities, they're just proportional to the true probability. This is fine though, it just means rather than 1-Pspam necessarily equalling Pham, we just go with whichever of Pspam and Pham is larger.\n","  # Get prediction\n","  if Pspamx > Phamx:  # Ties break for ham, because assignment doesn't say which way to do it and I feel like IRL you'd want to see the email if it was a tossup\n","    return \"SPAM\" # We could be clean and use a boolean here but I feel like it'll make later code easier to maintain non-confusing if-equals-ability :D\n","  else:\n","    return \"HAM\""],"metadata":{"id":"yjisJAElPhv2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Okay a quick followup and justification on that comment and why assigning a probability of 0 to words not in the dictionary is such a bad idea, and why I just did it anyway:\n","\n"," Since all our $ P(x_i|spam) $ and $ P(x_i|ham) $ probabilities for each word $ i $ in the email are getting multiplied together, any 0s in there are going to COMPLETELY defeat the rest of the probabilities, and return a $ P(spam|x_i) $ or $ P(ham|x_i) $ probability of 0, even if it's just one word that isn't in the dictionary! This is obviously a big drag on the model, but it's how it's shown in the lecture slides so it's how I implemented it. It may be mitigated somewhat by the fact that we take our spam and ham probabilities and go with the larger, but that remains to be seen."],"metadata":{"id":"cER0w6WZVjzs"}},{"cell_type":"markdown","source":["### Predictions"],"metadata":{"id":"AMWKcVKQWyCC"}},{"cell_type":"markdown","source":["Okay let's see if this thing works!"],"metadata":{"id":"tBAu-6MBW0nl"}},{"cell_type":"code","source":["# Becase this is our from-scratch model, we control what information the model actually looks at, that means we don't need any complicated accuracy measuring with a combined spam and ham dataset like we would have in the last two questions, we can just do a simple version of that ourselves and save the time fussing with numpy.\n","\n","corrects=0\n","misses=0\n","\n","corrects_train=0\n","misses_train=0\n","\n","# Testing Accuracy\n","for email in spam_emails_test:\n","  result = isSpam(email)\n","  if result==\"SPAM\":\n","    corrects += 1\n","  elif result == \"HAM\":\n","    misses += 1\n","  else:\n","    print(\"ERROR: Something is *very* messed up here!\")\n","\n","for email in ham_emails_test:\n","  result = isSpam(email)\n","  if result==\"HAM\":\n","    corrects += 1\n","  elif result==\"SPAM\":\n","    misses+=1\n","  else:\n","    print(\"ERROR: Something is *very* messed up here!\")\n","\n","# Restore our clean training dataset\n","spam_emails_train = spam_emails[:sdivider]\n","ham_emails_train = ham_emails[:hdivider]\n","\n","# Training Accuracy\n","for email in spam_emails_train:\n","  result = isSpam(email)\n","  if result==\"SPAM\":\n","    corrects_train += 1\n","  elif result == \"HAM\":\n","    misses_train += 1\n","  else:\n","    print(\"ERROR: Something is *very* messed up here!\")\n","\n","for email in ham_emails_train:\n","  result = isSpam(email)\n","  if result==\"HAM\":\n","    corrects_train += 1\n","  elif result==\"SPAM\":\n","    misses_train+=1\n","  else:\n","    print(\"ERROR: Something is *very* messed up here!\")\n","\n","# Sanity check!\n","if (corrects+misses+corrects_train+misses_train) != (len(spam_emails_test)+len(spam_emails_train)+len(ham_emails_test)+len(ham_emails_train)):\n","  print(\"ERROR: We appear to have lost some? :D\")\n","\n","# Do math and output results\n","training_accuracy = corrects_train/(corrects_train+misses_train)\n","testing_accuracy = corrects/(corrects+misses)\n","\n","print(\"Training Accuracy:\", training_accuracy,\"\\nTesting Accuracy:\", testing_accuracy)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y_arBU1fWzyc","executionInfo":{"status":"ok","timestamp":1667729969779,"user_tz":480,"elapsed":1569,"user":{"displayName":"Chris McLaughlin","userId":"07791563133797571791"}},"outputId":"6f468b57-4494-493e-e23a-e8a17b40474e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training Accuracy: 0.9119699409554483 \n","Testing Accuracy: 0.4492753623188406\n"]}]},{"cell_type":"markdown","source":["Okaaaay! So we have solidly decent training accuracy, while testing accuracy is literally worse than if we had just flipped a coin.\n","\n","So the model isn't great. Right out of the gate this looks like overfitting, which makes sense given our model literally does not have the ability to handle words that didn't come up in the training data, and we didn't have all *that* many emails in our dataset to begin with.\n","\n","The fact that we use a product for our probabilities and consider any words that don't come up in the relevant dictionary as $ P(x_i) = 0 $ probably isn't helping here - I *did* get curious and google this, apparently there's a technique called fuzzing that counteracts the negative effects of that, but that's a project for another time!"],"metadata":{"id":"OCxn0a41bxwY"}}],"metadata":{"kernelspec":{"display_name":"Python 3.7.7 64-bit","language":"python","name":"python37764bit7ab3a842c56f47fc9beb3e23e6b5e94d"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"},"colab":{"provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}